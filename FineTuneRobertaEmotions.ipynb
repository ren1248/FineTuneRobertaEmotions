{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fda8e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renis\\anaconda3\\envs\\test_env\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7384665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labelnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow, this is absolutely breathtaking! You're i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I can't get over how amazing your skills are. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the kind of content that makes me love...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm in awe of your creativity. How do you come...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You've outdone yourself once again. I have so ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labelnum\n",
       "0  Wow, this is absolutely breathtaking! You're i...         0\n",
       "1  I can't get over how amazing your skills are. ...         0\n",
       "2  This is the kind of content that makes me love...         0\n",
       "3  I'm in awe of your creativity. How do you come...         0\n",
       "4  You've outdone yourself once again. I have so ...         0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load your dataset\n",
    "#df1 = pd.read_csv(\"train.tsv\",sep='\\t', header = None, names = ['text', 'labelnum','id'])\n",
    "#df2 = pd.read_csv(\"test.tsv\",sep='\\t', header = None, names = ['text', 'labelnum','id'])\n",
    "df = pd.read_csv(\"GoEmo.csv\", sep = ',', header = None, names = ['text', 'labelnum'])\n",
    "#df = pd.concat([df1, df2])\n",
    "#del df['id']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff27ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#mask = df['labelnum'].str.contains(',', na = False)\n",
    "#df = df[~mask]\n",
    "#mask2 = df['labelnum'].str.contains('27', na = False)\n",
    "#df = df[~mask2]\n",
    "df['labelnum'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade27459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can take less than 50 records of each emotion by changing number in this function\n",
    "#def subset_data(group):\n",
    "#    return group.sample(min(50, len(group)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3ef83a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text  labelnum\n",
      "0     Wow, this is absolutely breathtaking! You're i...         0\n",
      "1     I can't get over how amazing your skills are. ...         0\n",
      "2     This is the kind of content that makes me love...         0\n",
      "3     I'm in awe of your creativity. How do you come...         0\n",
      "4     You've outdone yourself once again. I have so ...         0\n",
      "...                                                 ...       ...\n",
      "1395  Thumbs up if your emotional barometer is right...        27\n",
      "1396  Watching this feels like sailing on the sea of...        27\n",
      "1397  Thumbs up if you're leaving this video with a ...        27\n",
      "1398  This content is like a neutral background nois...        27\n",
      "1399  Thumbs up if your emotional state is cruising ...        27\n",
      "\n",
      "[1400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each group\n",
    "#subset_df = df.groupby('labelnum', group_keys=False).apply(subset_data)\n",
    "# Display the resulting subset dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c26db278",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set your hyperparameters\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-7\n",
    "epochs = 3\n",
    "num_warmup_steps = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f55be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# Define your dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length, num_labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = torch.zeros(self.num_labels)  # Initialize with zeros for all labels\n",
    "        label_indices = [int(idx) for idx in str(self.labels[idx]).split(',')]\n",
    "        label[label_indices] = 1  # Set the corresponding label indices to 1\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "574978f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"SamLowe/roberta-base-go_emotions\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d7b8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your data\n",
    "train_texts = df['text'].tolist()\n",
    "train_labels = df['labelnum'].tolist()\n",
    "train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=128, num_labels=28)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3123ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\renis\\anaconda3\\envs\\test_env\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=lr, eps=adam_epsilon, correct_bias=False)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=len(train_dataloader) * epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4149f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [07:25<00:00,  5.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Average Training Loss: 0.045116644649004396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [07:31<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 - Average Training Loss: 0.022130377156744627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 88/88 [07:32<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 - Average Training Loss: 0.016966043325903065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_roberta_go_emotions_new\\\\tokenizer_config.json',\n",
       " 'fine_tuned_roberta_go_emotions_new\\\\special_tokens_map.json',\n",
       " 'fine_tuned_roberta_go_emotions_new\\\\vocab.json',\n",
       " 'fine_tuned_roberta_go_emotions_new\\\\merges.txt',\n",
       " 'fine_tuned_roberta_go_emotions_new\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss, roc_auc_score, average_precision_score\n",
    "\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "#Re-run below code for different metrics, loss functions and evaluations as mentioned in csv\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = loss_fn(logits, labels)\n",
    "        #loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} - Average Training Loss: {average_loss}')\n",
    "\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_roberta_go_emotions_new\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_roberta_go_emotions_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31318541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0283cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "model_path = \"fine_tuned_roberta_go_emotions_new\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ecb0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"Watching this video was exhilirating. The ultimate adrenaline rush.\"\n",
    "encoding = tokenizer(test_text, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d5b326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(**encoding).logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c7132d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "predicted_labels = (torch.sigmoid(logits) > threshold).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb8152cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b33057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given ordered list of emotions\n",
    "emotion_labels = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \n",
    "    \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \n",
    "    \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83897151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Emotion Label: ['excitement']\n"
     ]
    }
   ],
   "source": [
    "# Convert the predicted labels tensor to a list of emotions\n",
    "predicted_labels_list = [emotion_labels[i] for i, value in enumerate(predicted_labels[0]) if value == 1]\n",
    "\n",
    "# Print the encoded emotion labels\n",
    "print(\"Encoded Emotion Label:\", predicted_labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b7b8bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Emotion Labels: excitement\n"
     ]
    }
   ],
   "source": [
    "# Convert the predicted labels tensor to a string of emotions\n",
    "predicted_labels_str = \", \".join(emotion_labels[i] for i, value in enumerate(predicted_labels[0]) if value == 1)\n",
    "\n",
    "# Print the encoded emotion labels\n",
    "print(\"Encoded Emotion Labels:\", predicted_labels_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "787ad67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch, os\n",
    "import googleapiclient.discovery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfd8c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"fine_tuned_roberta_go_emotions_new\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = [\n",
    "    \"admiration\", \"amusement\", \"anger\", \"annoyance\", \"approval\", \"caring\", \"confusion\", \n",
    "    \"curiosity\", \"desire\", \"disappointment\", \"disapproval\", \"disgust\", \"embarrassment\", \n",
    "    \"excitement\", \"fear\", \"gratitude\", \"grief\", \"joy\", \"love\", \"nervousness\", \"optimism\", \n",
    "    \"pride\", \"realization\", \"relief\", \"remorse\", \"sadness\", \"surprise\", \"neutral\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae1c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a function for emotion prediction\n",
    "def predict_emotion(comment, threshold=0.5):\n",
    "    encoding = tokenizer(comment, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(**encoding).logits\n",
    "    \n",
    "    # Convert logits to binary predictions based on the threshold\n",
    "    predicted_labels = (torch.sigmoid(logits) > threshold).int()\n",
    "    \n",
    "    # Convert the predicted labels tensor to a string of emotions\n",
    "    predicted_labels_str = \", \".join(emotion_labels[i] for i, value in enumerate(predicted_labels[0]) if value == 1)\n",
    "    \n",
    "    return predicted_labels_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5244513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_youtube_comments(video_url, api_key):\n",
    "    # Extract video ID from the URL\n",
    "    video_id = video_url.split(\"v=\")[1]\n",
    "\n",
    "    # Create a YouTube API client\n",
    "    youtube = googleapiclient.discovery.build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    # Get video details\n",
    "    video_response = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    video_title = video_response[\"items\"][0][\"snippet\"][\"title\"]\n",
    "\n",
    "    # Get comments\n",
    "    comments = []\n",
    "    nextPageToken = None\n",
    "\n",
    "    while True:\n",
    "        comment_response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=500,  # Adjust as needed\n",
    "            pageToken=nextPageToken\n",
    "        ).execute()\n",
    "\n",
    "        for item in comment_response[\"items\"]:\n",
    "            comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            comments.append(comment)\n",
    "\n",
    "        nextPageToken = comment_response.get(\"nextPageToken\")\n",
    "\n",
    "        if not nextPageToken:\n",
    "            break\n",
    "\n",
    "    return video_title, comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8dbcc965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Title: Probability Part 1: Rules and Patterns: Crash Course Statistics #13\n",
      "Total Comments: 143\n",
      "\n",
      "Comments:\n",
      "1. Wow\n",
      "2. I stg I&#39;ve learned more from CrashCourse than I have from my community college professors. I&#39;ve gotta become a patreon, you guys have saved my butt so many times.\n",
      "3. <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=4\">0:04</a> ADRIENE!!!! My favorite teacher!!!!\n",
      "4. Omg ‚ù§Ô∏è\n",
      "5. <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=342\">5:42</a> &quot;you like like cole sprouse&quot; what does that mean?\n",
      "6. demnnn probablity was made this quick this was amazing\n",
      "7. I didn&#39;t start seeing faces until this person brought it up. Now I can&#39;t stop.\n",
      "8. items on the table keeps getting changed as per the topic üòÅ\n",
      "9. ü§Ø whats the probability I will drop my 6 week summer stats class? Id rather take Anatomy againüò©\n",
      "10. Too fast with explanations...rather too many words for few simple explanation.\n",
      "11. Cole shouldn&#39;t  go to the IHOP I worked at...wouldn&#39;t recommend it üòÇüòÇüòÇ\n",
      "12. idk\n",
      "13. can you guys slow down when teaching at crash course? you are really helpful, but you are bombarding me with words at a tremendous speed!\n",
      "14. Aaaaaaaaannnnd now I have no idea what&#39;s going on.\n",
      "15. I didn&#39;t understand anything in class because my teacher talks so fast but I finally get it, even though I&#39;d already failed his test XD\n",
      "16. I appreciate people who take the time to put learning videos online, especially math videos. But I must say the pace of this video is too fast. I saw too many imponderables during the first minute of the video where I had to pause and go back as well as the rest of the video. I understood everything after stopping and rewinding but slow down a bit.\n",
      "17. If P(1 and 2)= P(1) x P(2) then why isn&#39;t P(1|2) not just equal P(1) since it&#39;s said that P(1|2) = ( P(1 and 2) / P(1) ) ???\n",
      "18. Maam, can you provide the solutions to your questions? all of them for reinforcement? thank you\n",
      "19. I think that when calculating the probability of P(A or B), we shouldn&#39;t calculate the probability of P(A and B) at all. So I think the formula should be like this: P(A or B) = P(A) + P(B) - 2P(A and B). That way, we can calculate the probability of only A or B occurring. The video says P(A or B) = P(A) + P(B) - P(A and B). Maybe our definition of the word &#39;or&#39; is different? Please correct me if I&#39;m wrong.\n",
      "20. At <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=232\">3:52</a>, I think what Adriene meant should be &quot;Since P(tails and 6) is not equal to zero, these two events are not mutually exclusive&quot; instead of &quot;Since P(tails or 6) is not equal to zero,...&quot;\n",
      "21. How did you get the P(cole and ice cream=0.02?\n",
      "22. Non popular examples, big sentences, unnecessary adjunctives ... But skipping complicated explanations.\n",
      "23. <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=18\">0:18</a> It&#39;s a bit more general. &quot;The tendency to perceive a specific, often meaningful image in a random or ambiguous visual pattern&quot;\n",
      "24. I don&#39;t like all these pop culture references. Like all the songs at the end of another episode or Cole Sprouse (I have no idea who that is), or even IHOP - here in Europe there is no IHOP (at least I haven&#39;t seen any), so when this happens I&#39;m sitting like &quot;Oookay, whatever.&quot; (And yes, I know I don&#39;t have to be familiar with all these things in order to understand the video. I&#39;m just saying that it slightly annoys me.)\n",
      "25. <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=298\">4:58</a> is the reason why youtube channels like this, most of the time, win over teachers\n",
      "26. The comparisons were so bizarre it almost made sense but then she had to make a racist comment and completely lost me.\n",
      "27. Please don‚Äôt use examples of people who some of older folks don‚Äôt know or care about.  Using Cole whatever was distracting and not helpful.  Otherwise your information is so helpful.\n",
      "28. I had to google for like 2 minutes to figure out what the heck a &#39;mono&#39; is.\n",
      "29. Just something about her voice man....\n",
      "30. thank you for this video. It has helped me immensely!\n",
      "31. How does probability relate to eclectic theoretical orientation?\n",
      "32. I still don&#39;t understand\n",
      "33. I think that at <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=253\">4:13</a> the title and the formula are not matching, the title says the prob of tails AND 6, but the formula says p(tails OR 6). I am confused...\n",
      "34. Lost me at &quot;Medical Screenings&quot;\n",
      "35. I really don&#39;t like the way she talks\n",
      "36. i got a 61..... yaay\n",
      "37. Now I understand why I fail because I sleep every time we talk about math\n",
      "38. Thanks for this educational video that i get to watch in class\n",
      "39. This is one of the best reviews of basic probability I&#39;ve seen. Thanks Ms. Hill!\n",
      "40. i think you should use writting and explanations (handnotes) to better explain this.\n",
      "41. Excellent .\n",
      "42. You are too fast Mrs. Tutor\n",
      "43. Great videos\n",
      "44. <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=228\">3:48</a> Since P(tails AND 6) != 0\n",
      "45. Wow, very clear explanations. Good job!\n",
      "46. What&#39;s the probability I&#39;ll pass my stats exam?\n",
      "47. What???\n",
      "48. At <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=231\">3:51</a> you mean P(tailsAnd6)...\n",
      "49. Finally. The first 13 videos was pretty simple. Now I feel like actually learning somethign that i wasnt taught in 3rd-5th grade.\n",
      "50. The Venn diagram that has No Cole overlapping &#39;Cole&#39; and also completely overlapping &#39;ice cream&#39; is a bit confusing. Am I missing something?\n",
      "51. Yall shouldn‚Äôt put hot teachers\n",
      "52. you lost me at p(Cole) and p(ice cream night)\n",
      "53. By refering to Cole, did you mean Jughed? Noticed the beanie haha\n",
      "54. Thanks a LOT!!! Now I want ice cream AND feel compelled to catch up on Riverdale:( Riverdale, can you forgive me?\n",
      "55. Nothing gets me harder than statistics! For real.\n",
      "56. What happened to skittles?\n",
      "57. ...?!\n",
      "58. Look Man!<br>Plz try to make it a little more simpler to understand!!\n",
      "59. You summed up an hour class in 12 minutes. Thank you so much for the great review.\n",
      "60. &quot;A skittle can&#39;t be two different colors at once.&quot;  Schrodinger watching this shaking his head.*\n",
      "61. Here is when CrashCourse turns into CryingCourse.\n",
      "62. I think you meant to say P(tails and 6) at <a href=\"https://www.youtube.com/watch?v=OyddY7DlV58&amp;t=228\">3:48</a>\n",
      "63. Not judging, but must admit, the comments below made me curious to watch. Good coverage, easy to relate. Great job. Thank you.\n",
      "64. What‚Äôs the probability she has a black boyfriend\n",
      "65. I approve of this much Cole in a Crash Course Video ^^\n",
      "66. Please slow down...............\n",
      "67. HELP A LOT THANK YOU PLEASE DONT STOP\n",
      "68. Good video,¬† It helped me understand the concept,¬† I minor criticism though,¬† go a little slower with the slides\n",
      "69. This is amazing animated educational material. I am deeply touched\n",
      "70. the probability of me ever watching this video again is pretty low. :P\n",
      "71. Don&#39;t you mean ihob\n",
      "72. Ah my brain hurts\n",
      "73. Where is #12??\n",
      "74. I H O B\n",
      "75. Adriene looks more confident since the economics crash course with Mr Clifford. I really enjoyed this presentation; she talks slower and is less much tense. Thank you for doing what you do, you are the reason decided to study economics!\n",
      "76. wow just noticed that the plant on the left, is in the shape of a histogram of a distribution. Very clever crash course, that was a cool thing to do.\n",
      "77. Can we make absolute deductions of certainty when we&#39;re talking about what is probable, or is it more so estimaTion? Is there 100% probability, is thaT no longer talking of probability?\n",
      "78. stats exam is tomorrow im currently binging this\n",
      "79. Good stuff.\n",
      "80. I settled for front/left to see BP.\n",
      "81. If you want to know the probability of hitting a jackpot on a slotmachine, 100 games ( I know, just a number - still) is not a sufficient sampling size. Try 100.000.000 games, that is what it&#39;s usually tested with. Source: I work in the industry.\n",
      "82. pee of cole\n",
      "83. See a doctor if you pee red or purple.\n",
      "84. There was a good couple of minutes there in the middle where i could have sworn i was watching the &quot;Engineer guy&quot; with a wig on talking about probability. The tone and mannerisms got really similar.\n",
      "85. I see faces in cars\n",
      "86. I&#39;ve passed probabity on a university level, so i feel kinda smart watching this\n",
      "87. bayesian is not happy\n",
      "88. This episode could be an entire crash course.  What are the chances of there being a crash course probability?\n",
      "89. Good job!\n",
      "90. We are learning about this in school right now\n",
      "91. Theres a number of eateries where 20% roll a mental dice each day is probably not a bad approach for modelling me.\n",
      "92. but what&#39;s the probability of Dylan at ihop?\n",
      "93. lol I had a math quiz about this today.\n",
      "94. No idea who Cold Sprouts or an IHOP is. By trying to be relatable to American kids you&#39;re alienating the rest of the world\n",
      "95. I do not mean to be rude but I found this very confusing for some reason.......and I have understood everything up until now\n",
      "96. Uhm... That whole Cole Sprouse thing was pretty funny not gonna lie\n",
      "97. <b>HELLO GUYSS!</b> Help me to reach 14.5K <b>subscribers</b> on my channel i really appreciate  those people who help &amp; particpate to reach my GOAL‚úî thanks it really means a lotüáµüá≠üòÑüòÅüíï\n",
      "98. Crash course rocks like if you agree\n",
      "99. I was with you till the part you said you &quot;don&#39;t want your weekend to suck&quot; but then said &quot;you could stream get out&quot; I mean, which is it... sucky weekend with get out, or have a decent to good weekend? =p\n",
      "100. best episode yet\n",
      "101. finally some maths\n",
      "102. Excellent! Just one thing: P(A|B) does not require Event B to occur before Event A.\n",
      "103. will you guys go over Random Variables and Probability Density functions and such?\n",
      "104. Your videos are extremely well produced and informative, thank  you for this content :)\n",
      "105. My statistics class just started on probability and one of the discussion assignments is to share videos of something we are having a hard time with. I think I understand all the basic concepts in this video, so I wish the rest of this series was posted because I would have loved to have used Crash Course!\n",
      "106. The probability of meeting another human is zero, population of the universe divide by the space of the universe.\n",
      "107. I wanna dump out that jar of D6&#39;s and yell, &quot;FIREBALL!&quot; :x\n",
      "108. Yiu should make a video on English Language.\n",
      "109. I enjoy logical conclusions. Presumptions, deny lying eyes.\n",
      "110. Thank you so much! this is exactly what I needed, my test is in a day!!!\n",
      "111. What about brown and black Skittles? You racist.\n",
      "112. Where are there theaters where it‚Äôs not general seating? Never seen that.\n",
      "113. Don&#39;t Forget To Be Asking Questions.\n",
      "114. Thanks for explaining in terms of ice cream and Cole Sprouse, I love the Internet.\n",
      "115. Finally!  Math!\n",
      "116. You guys post this after my exam!!!<br>Like less than 8 hours after it...\n",
      "117. There is a large probability that I‚Äôm going to kms during my finals\n",
      "118. Woah double upload XD\n",
      "119. Thanks for reminding me how dumb I am.\n",
      "120. wtf is cole\n",
      "121. hi\n",
      "122. Why don‚Äôt they do math?\n",
      "123. Very good!\n",
      "124. I think the p(cancer) example is the best part of this series I&#39;ve heard so far. It immediately explains the importance of the field.\n",
      "125. I have a chemistry exam after 5 hours (FINALS!!!) but look at me... -_-\n",
      "126. <b><i>...sidebar: M&amp;M&#39;s white chocolates also have five colors‚Äîwhy is five best for statistics...</i></b>\n",
      "127. can we get into tutorials now?\n",
      "128. In crash course mythology, where would u find the myths in full versions online?\n",
      "129. Yo\n",
      "130. <b><i>...&#39;ooh-ooh&#39;‚Äîwhat&#39;s &#39;antipareidolia&#39; called‚Äîthe &#39;inability&#39; of a murderer to see the corpse...</i></b>\n",
      "131. You guys are amazing no matter whatever topic you make a video on, keep up the great work\n",
      "132. Yessss cole and icecream\n",
      "133. Now this was presented really well.\n",
      "134. Wow I&#39;ve never thought that I would probably be this early to watch this\n",
      "135. After 42 seconds I figured this video will include a little spiel on discrediting  conspiracies. Now to watch the rest of the video.  I watched it and was pleasantly surprised she stuck to the topic.\n",
      "136. OMG I JUST LEARNED ABOUT PROBABILITY TODAY IN MY GEOMETRY CLASS AND THIS VIDEO CAME OUT TODAY WTH!!H!H!?!?!?!\n",
      "137. Maybe now I&#39;ll get likes on my comment\n",
      "138. 7 th\n",
      "139. was really hoping she was going to say &quot;grilled Cheesus&quot;\n",
      "140. It&#39;s so simple\n",
      "141. It was very improbable that I was so early to this video.\n",
      "142. Great video!\n",
      "143. Cool\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'YOUR_API_KEY' with your actual YouTube API key\n",
    "    api_key = 'AIzaSyAlV9EbT68jAJuo_Fk5XklaeKzN1Uxj0B0'\n",
    "\n",
    "    # Replace 'YOUR_VIDEO_URL' with the YouTube video URL\n",
    "    video_url = 'https://www.youtube.com/watch?v=OyddY7DlV58'\n",
    "\n",
    "    video_title, comments = get_youtube_comments(video_url, api_key)\n",
    "\n",
    "    print(f\"Video Title: {video_title}\")\n",
    "    print(f\"Total Comments: {len(comments)}\")\n",
    "    print(\"\\nComments:\")\n",
    "    for i, comment in enumerate(comments, start=1):\n",
    "        print(f\"{i}. {comment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6cb1c6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty DataFrame\n",
    "dfin = pd.DataFrame(columns=['Label', 'Comment'])\n",
    "\n",
    "# Process comments and store results in the DataFrame\n",
    "for i, comment in enumerate(comments, start=1):\n",
    "    predicted_emotions = predict_emotion(comment)\n",
    "    \n",
    "    # Append results to the DataFrame\n",
    "    dfin = pd.concat([dfin, pd.DataFrame({ 'Label': [predicted_emotions], 'Comment': [comment]})])\n",
    "\n",
    "# Sort the DataFrame by 'Label'\n",
    "df_sorted = dfin.sort_values(by='Label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8b03cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                            Comment\n",
      "0                                                          Cool\n",
      "0                                                   Good stuff.\n",
      "0              stats exam is tomorrow im currently binging this\n",
      "0             I don&#39;t like all these pop culture referen...\n",
      "0             Adriene looks more confident since the economi...\n",
      "..       ...                                                ...\n",
      "0    sadness  Here is when CrashCourse turns into CryingCourse.\n",
      "0   surprise  Wow I&#39;ve never thought that I would probab...\n",
      "0   surprise                              Woah double upload XD\n",
      "0   surprise  After 42 seconds I figured this video will inc...\n",
      "0   surprise                                                Wow\n",
      "\n",
      "[143 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print or save the sorted DataFrame\n",
    "print(df_sorted)\n",
    "#df_sorted.to_csv(\"predicted_comments_new.csv\", index=False)  # Save to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b39e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
